{
  "input": null,
  "output": null,
  "pipeline": null,
  "components": [
    {
      "componentInstance": "sqlExportLexicalEntries",
      "class": "SQLStreamTransformer",
      "driver": null,
      "connectUrl": "jdbc:mysql://localhost:3306/lexisdb",
      "user": "root",
      "password": "<$param0>",
      "query": "scripts/exportLexicalEntries.sql",
      "outFormat": "TSV",
      //the following will be ignored, since outFormat is defined.
      "escapeChar": null,
      "delimiterCSV": "\t",
      "quoteChar": null,
      "emptyChar": "_"
    },
    {
      "componentInstance": "tarqlGramcatMapping",
      "class": "TarqlStreamTransformer",
      "query": "scripts/gramcatConstructMapping.tarql",
      "delimiterCSV": ",",
      "tabs": "no",
      "quoteChar": null,
      "escapeChar": null,
      "encoding": "UTF-8",
      "headerRow": true,
      "baseIRI": null,
      "write-base": null,
      "dedup": "0"
    },
    {
      "componentInstance": "duplicateTarqlResults",
      "class": "IOStreamDuplicator"
    },
    {
      "componentInstance": "loadSplitTTL",
      "class": "RDFStreamLoader",
      "lang": "Turtle",
      "globalPrefixes": true,
      //split after every linebreak
      "delimiter": null,
      "split": true
    },
    {
      "componentInstance": "loadBulkTTL",
      "class": "RDFStreamLoader",
      "lang": "Turtle",
      "globalPrefixes": true,
      //never split
      "delimiter": null,
      "split": false
    },
    {
      "componentInstance": "updatePOS",
      "class": "RDFUpdater",
      "threads": "0",
      "lookahead": "0",
      "lookback": "0",
      "updates": [
        {
          "path": "scripts/gramcatUpdatePOS.sparql",
          "iter": 1
        }
      ]
    },
    {
      "componentInstance": "writeTTL",
      "class": "RDFStreamWriter",
      "lang": "Turtle",
      "delimiter": null,
      "prefixDeduplication": true
    }
  ],
  "streams": [
    // Stream 1
    // read lexical entries generated by SQL query into Loader
    {
      "readsFromInstance": "sqlExportLexicalEntries",
      "writesToInstance": "loadSplitTTL",
      "writesToInstanceGraph": "lexicalEntries"
    },
    // Load lexical entries to Updater
    {
      "readsFromInstance": "loadSplitTTL",
      "readsFromInstanceGraph": "lexicalEntries",
      "writesToInstance": "updatePOS",
      "writesToInstanceGraph": ""
    },
    // Stream 2
    // Read Mapping file into TARQL
    {
      "readsFromSource": "data/gramcat.csv",
      "writesToInstance": "tarqlGramcatMapping",
      "writesToInstanceGraph": ""
    },
    // Load the TARQL results into duplicator
    {
      "readsFromInstance": "tarqlGramcatMapping",
      "readsFromInstanceGraph": "",
      "writesToInstance": "duplicateTarqlResults",
      "writesToInstanceGraph": ""
    },
    // (OPTIONAL) Export TARQL results as a transformed mapping file
    {
      "readsFromInstance": "duplicateTarqlResults",
      "readsFromInstanceGraph": "",
      "writesToDestination": "LexisFinalRDF/exportedMapping.ttl"
    },
    // Load the TARQL results into segmented Graph
    {
      "readsFromInstance": "duplicateTarqlResults",
      "readsFromInstanceGraph": "",
      "writesToInstance": "loadBulkTTL",
      "writesToInstanceGraph": "gramcatMapping"
    },
    // Side-Load segmented Graph to Updater as named Graph <http://mapping>
    {
      "readsFromInstance": "loadBulkTTL",
      "readsFromInstanceGraph": "gramcatMapping",
      "writesToInstance": "updatePOS",
      "writesToInstanceGraph": "http://mapping"
    },
    // Stream 3:    from here the streams are merged
    // Pipe results of updater into RDF Writer
    {
      "readsFromInstance": "updatePOS",
      "readsFromInstanceGraph": "",
      "writesToInstance": "writeTTL",
      "writesToInstanceGraph": ""
    },
    // Export Output file
    {
      "readsFromInstance": "writeTTL",
      "readsFromInstanceGraph": "",
      "writesToDestination": "LexisFinalRDF/lexis.ttl"
    }
  ]
}
